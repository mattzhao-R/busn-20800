{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/mzhao117/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun:good\n",
      "noun:good,goodness\n",
      "noun:good,goodness\n",
      "noun:commodity,trade_good,good\n",
      "adj:good\n",
      "adj(s):full,good\n",
      "adj:good\n",
      "adj(s):estimable,good,honorable,respectable\n",
      "adj(s):beneficial,good\n",
      "adj(s):good\n",
      "adj(s):good,just,upright\n",
      "adj(s):adept,expert,good,practiced,proficient,skillful,skilful\n",
      "adj(s):good\n",
      "adj(s):dear,good,near\n",
      "adj(s):dependable,good,safe,secure\n",
      "adj(s):good,right,ripe\n",
      "adj(s):good,well\n",
      "adj(s):effective,good,in_effect,in_force\n",
      "adj(s):good\n",
      "adj(s):good,serious\n",
      "adj(s):good,sound\n",
      "adj(s):good,salutary\n",
      "adj(s):good,honest\n",
      "adj(s):good,undecomposed,unspoiled,unspoilt\n",
      "adj(s):good\n",
      "adv:well,good\n",
      "adv:thoroughly,soundly,good\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "poses = {'n':'noun','v':'verb','s':'adj(s)','a':'adj','r':'adv'}\n",
    "for synset in wn.synsets('good'):\n",
    "    print(\"{}:{}\".format(poses[synset.pos()],\n",
    "                         \",\".join([l.name() for l in synset.lemmas()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun:bank\n",
      "noun:depository_financial_institution,bank,banking_concern,banking_company\n",
      "noun:bank\n",
      "noun:bank\n",
      "noun:bank\n",
      "noun:bank\n",
      "noun:bank,cant,camber\n",
      "noun:savings_bank,coin_bank,money_box,bank\n",
      "noun:bank,bank_building\n",
      "noun:bank\n",
      "verb:bank\n",
      "verb:bank\n",
      "verb:bank\n",
      "verb:bank\n",
      "verb:bank\n",
      "verb:deposit,bank\n",
      "verb:bank\n",
      "verb:trust,swear,rely,bank\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "poses = {'n':'noun','v':'verb','s':'adj(s)','a':'adj','r':'adv'}\n",
    "for synset in wn.synsets('bank'):\n",
    "    print(\"{}:{}\".format(poses[synset.pos()],\n",
    "                         \",\".join([l.name() for l in synset.lemmas()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('procyonid.n.01'),\n",
       " Synset('carnivore.n.01'),\n",
       " Synset('placental.n.01'),\n",
       " Synset('mammal.n.01'),\n",
       " Synset('vertebrate.n.01'),\n",
       " Synset('chordate.n.01'),\n",
       " Synset('animal.n.01'),\n",
       " Synset('organism.n.01'),\n",
       " Synset('living_thing.n.01'),\n",
       " Synset('whole.n.02'),\n",
       " Synset('object.n.01'),\n",
       " Synset('physical_entity.n.01'),\n",
       " Synset('entity.n.01')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "panda = wn.synset(\"panda.n.01\")\n",
    "hyper = lambda s: s.hypernyms()\n",
    "list(panda.closure(hyper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We8there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis.gensim_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8a52e95a2762>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# in terminal: python -m spacy download en_core_web_md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m \u001b[0;31m# pip install pyldavis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgensim_models\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgensimvis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis.gensim_models'"
     ]
    }
   ],
   "source": [
    "import gensim # pip install gensim\n",
    "import spacy # pip install spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "# in terminal: python -m spacy download en_core_web_md\n",
    "import pyLDAvis # pip install pyldavis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import re\n",
    "import string\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer # pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'we8there_texts.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c9d27c4eabd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"we8there_texts.npy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallow_pickle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib64/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'we8there_texts.npy'"
     ]
    }
   ],
   "source": [
    "texts = np.load(\"we8there_texts.npy\",allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary     = gensim.corpora.Dictionary(texts)\n",
    "bow_corpus     = [dictionary.doc2bow(doc) for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running LDA using Bag of Words\n",
    "lda_model = gensim.models.LdaMulticore(corpus=bow_corpus,id2word=dictionary,num_topics=10, passes=20,random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the topics we've found\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    vis_data1 = gensimvis.prepare(lda_model, bow_corpus, dictionary)\n",
    "pyLDAvis.display(vis_data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all topics: we choose topic 10 which means each topic concludes 10 variables.\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trump&Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"trump.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out how many tweets we collected\n",
    "text_data = list(df[\"text\"].values)\n",
    "date      = list(df[\"date\"].values)\n",
    "T         = len(text_data) # 8411 tweets remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spacy_nlp   = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweet):\n",
    "    \"\"\"\n",
    "    Tokenize and lemmatize an input tweet\n",
    "    Args:\n",
    "        tweet: string\n",
    "    Returns:\n",
    "        a list containing tokens\n",
    "    \"\"\"\n",
    "    tweet       = re.sub('&amp;', ' ',tweet)\n",
    "    tweet       = emoji_pattern.sub(r' ', tweet)\n",
    "    word_tokens = spacy_nlp(tweet)\n",
    "    tokens      = []\n",
    "    for w in word_tokens:\n",
    "        if not w.is_stop: # not stop words\n",
    "            s   = w.text.lower()\n",
    "#            print(s)\n",
    "#            if s[0] == '@' or s[0] == '#':\n",
    "#                continue\n",
    "            s   = re.sub(r'^[@#]', '', s)\n",
    "            s   = re.sub(r'[^a-zA-Z0-9_]+$', '', s)\n",
    "            s   = re.sub(r'[-,#()@=!\\\"\\'\\?\\/:]+', ' ', s)\n",
    "            #replace consecutive non-ASCII characters with a space\n",
    "            s   = re.sub(r'[^\\x00-\\x7F]+',' ', s)\n",
    "            tokens += s.split()\n",
    "    text = \" \".join(tokens)\n",
    "#    print(tokens)\n",
    "    word_tokens    = spacy_nlp(text)\n",
    "    filtered_tweet = []\n",
    "    for w in word_tokens:\n",
    "        if not w.is_stop:\n",
    "            if w.lemma_ != \"-PRON-\":\n",
    "                s = w.lemma_.lower()\n",
    "            else:\n",
    "                s = w.lower_\n",
    "            s = s.strip('-')\n",
    "            if re.match(r'^[a-zA-Z_\\.]+$', s):\n",
    "                filtered_tweet.append(s)\n",
    "#            if s not in string.punctuation and not re.match(r\"^[0-9\\.]+$\", s) and re.match(r'[a-zA-Z_]+', s):\n",
    "#                filtered_tweet.append((s, w.pos_))\n",
    "    \n",
    "    return filtered_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag-of-words model\n",
    "# clean each tweet in text data and then store them as processed_docs\n",
    "processed_docs = []\n",
    "for t in text_data:\n",
    "    try: \n",
    "        processed_docs.append(clean_tweets(t))\n",
    "    except: pass\n",
    "\n",
    "\n",
    "# create a dictionary for all tweets\n",
    "dictionary     = gensim.corpora.Dictionary(processed_docs)\n",
    "# create word bow for all tweets, this can help us record the times of each word appeared in each tweet.\n",
    "bow_corpus     = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running LDA using Bag of Words\n",
    "lda_model = gensim.models.LdaMulticore(corpus=bow_corpus,id2word=dictionary,num_topics=10, passes=20,random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all topics: we choose topic 10 which means each topic concludes 10 variables.\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the topics we've found\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    vis_data1 = gensimvis.prepare(lda_model, bow_corpus, dictionary)\n",
    "pyLDAvis.display(vis_data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer(sentences, pos_cutoff = 0.7, neg_cutoff = -0.7):\n",
    "    \"\"\"\n",
    "    Given a list of tweets, return those that contain strong positive/negative sentiments\n",
    "    Args:\n",
    "        sentences: a list containing the indices of tweets in text_data\n",
    "        pos_cutoff: tweets with sentiment scores >= pos_cutoff are classified as positive\n",
    "        neg_cutoff: tweets with sentiment scores <= neg_cutoff are classified as negative\n",
    "    Returns:\n",
    "        pos: a list containing the indices of tweets that are classified as positive\n",
    "        neg: a list containing the indices of tweets that are classified as negative\n",
    "    \"\"\"\n",
    "    analyser  = SentimentIntensityAnalyzer()\n",
    "    pos, neg  = [], []\n",
    "    for i in sentences:\n",
    "        score = analyser.polarity_scores(text_data[i])\n",
    "        #print(\"{:-<40} {}\".format(s, str(score)))\n",
    "        if score['compound']   >= pos_cutoff: # positive comments\n",
    "            pos.append(i)\n",
    "        elif score['compound'] <= neg_cutoff: # negative comments\n",
    "            neg.append(i)\n",
    "    return (pos, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-ae118c6ab5b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment_analyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'text_data' is not defined"
     ]
    }
   ],
   "source": [
    "pos, neg = sentiment_analyzer(np.arange(len(text_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'neg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7c0a04ce1d79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'neg' is not defined"
     ]
    }
   ],
   "source": [
    "len(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive sentence example\n",
    "text_data[pos[20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negtive sentence example\n",
    "\n",
    "text_data[neg[10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie review example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "nltk.download('movie_reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict(pos = movie_reviews.fileids('pos'),\n",
    "            neg = movie_reviews.fileids('neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_text (clf,file_id,start=0,end=None):\n",
    "    words = list(movie_reviews.words(data[clf][file_id]))\n",
    "    return ' '.join(words[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive movie example\n",
    "print(get_review_text('pos',0,end=95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative movice example\n",
    "print(get_review_text('neg',0,start=61,end=191))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_features (words):\n",
    "    \"\"\"\n",
    "    This is the simplest possible feature representation of a document.\n",
    "\n",
    "    Each word is a feature.\n",
    "    \"\"\"\n",
    "    return dict((word, True) for word in words)\n",
    "\n",
    "\n",
    "def extract_features (corpus, file_ids, cls, feature_extractor=unigram_features):\n",
    "    \"\"\"\n",
    "    Turn a set of files all belonging to one class into a list\n",
    "    of (feature dictionary, cls) pairs, to be used in testing or training\n",
    "    a classifier.\n",
    "    \"\"\"\n",
    "    return [(feature_extractor(corpus.words(i)), cls) for i in file_ids]\n",
    "\n",
    "\n",
    "def get_words_from_corpus (corpus, file_ids):\n",
    "\n",
    "    for file_id in file_ids:\n",
    "        words = corpus.words(file_id)\n",
    "        for word in words:\n",
    "            yield word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 70% as training data and 30% as test data\n",
    "test_start_index = 700\n",
    "\n",
    "\n",
    "# Training set\n",
    "neg_training = extract_features(movie_reviews, data['neg'][:test_start_index], 'neg',\n",
    "                                feature_extractor=unigram_features)\n",
    "\n",
    "pos_training = extract_features(movie_reviews, data['pos'][:test_start_index],'pos',\n",
    "                                feature_extractor=unigram_features)\n",
    "\n",
    "train_set = pos_training + neg_training\n",
    "\n",
    "# Test set\n",
    "neg_test = extract_features(movie_reviews, data['neg'][test_start_index:], 'neg',\n",
    "                                feature_extractor=unigram_features)\n",
    "\n",
    "pos_test = extract_features(movie_reviews, data['pos'][test_start_index:],'pos',\n",
    "                                feature_extractor=unigram_features)\n",
    "\n",
    "test_set = pos_test + neg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conflict comments in one review (difficuly in the task)\n",
    "\n",
    "# Positive review\n",
    "print(\"****Positive review****\")\n",
    "print (get_review_text('pos',0,end=95))\n",
    "\n",
    "print ('\\n. . . . . . \\n')\n",
    "\n",
    "print(\"****Negative review****\")\n",
    "\n",
    "# Negative review at the same time\n",
    "print (get_review_text('pos',0,start=-190))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.classify(train_set[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score,accuracy_score\n",
    "\n",
    "def do_evaluation (pairs, pos_label='pos'):\n",
    "    predicted, actual = zip(*pairs)\n",
    "    (precision, recall,accuracy) = (precision_score(actual,predicted,pos_label=pos_label),\n",
    "                                    recall_score(actual,predicted,pos_label=pos_label),\n",
    "                                    accuracy_score(actual,predicted))\n",
    "\n",
    "    print_results(precision, recall, accuracy, pos_label)\n",
    "    \n",
    "\n",
    "def print_results (precision, recall, accuracy, pos_label):\n",
    "    banner =  'Evaluation with pos label = %s' % pos_label\n",
    "    print('\\n')\n",
    "    print(banner)\n",
    "    print ('=' * len(banner))\n",
    "    print ('{0:10s} {1:.1f}'.format('Precision',precision*100))\n",
    "    print ('{0:10s} {1:.1f}'.format('Recall',recall*100))\n",
    "    print ('{0:10s} {1:.1f}'.format('Accuracy',accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(classifier.classify(example), actual)\n",
    "            for (example, actual) in test_set]\n",
    "temp =np.array([pairs[i][0]==pairs[i][1]for i in range(len(pairs))])\n",
    "acc = np.sum(temp)/temp.shape[0]\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = SklearnClassifier(LogisticRegression()).train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [(classifier.classify(example), actual)\n",
    "            for (example, actual) in test_set]\n",
    "temp =np.array([pairs[i][0]==pairs[i][1]for i in range(len(pairs))])\n",
    "acc = np.sum(temp)/temp.shape[0]\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hotel review example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://kavita-ganesan.com/entity-ranking-data/#.YoQAR5qZPjg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use truncated SVD to reduce dimension\n",
    "def reduce_to_k_dim(M, k=2):  \n",
    "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\n",
    "    M_reduced = None\n",
    "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
    "\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    svd = TruncatedSVD(n_components = k, n_iter = n_iters,random_state = 1)\n",
    "    #a   = svd.fit(M)\n",
    "    M_reduced = svd.fit_transform(M)\n",
    "    print(\"Done.\")\n",
    "    return M_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matrix_of_vectors(wv_from_bin):\n",
    "    import random\n",
    "    words = list(wv_from_bin.key_to_index.keys())\n",
    "    print(\"Shuffling words ...\")\n",
    "    random.shuffle(words)\n",
    "    print(\"Putting %i words into word2Ind and matrix M...\" % len(words))\n",
    "    word2Ind = {}\n",
    "    M = []\n",
    "    curInd = 0\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(wv_from_bin.word_vec(w))\n",
    "            word2Ind[w] = curInd\n",
    "            curInd += 1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    M = np.stack(M)\n",
    "    print(\"Done.\")\n",
    "    return M, word2Ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embeddings(M_reduced, word2Ind, words):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize = (12,8))\n",
    "    for i,word in enumerate(words):\n",
    "        x = M_reduced[(word2Ind.get(word),0)]\n",
    "        y = M_reduced[(word2Ind.get(word),1)]\n",
    "        plt.scatter(x, y, marker='x', color='red')\n",
    "        plt.text(x+0.01, y+0.01, word, fontsize=18)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import logging\n",
    "\n",
    "data_file=\"reviews_data.txt.gz\"\n",
    "\n",
    "with gzip.open ('reviews_data.txt.gz', 'rb') as f:\n",
    "    for i,line in enumerate (f):\n",
    "        print(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "    \n",
    "    \n",
    "    with gzip.open (input_file, 'rb') as f:\n",
    "        for i, line in enumerate (f): \n",
    "\n",
    "            # do some pre-processing and return a list of words for each review text\n",
    "            yield gensim.utils.simple_preprocess (line)\n",
    "\n",
    "# read the tokenized reviews into a list\n",
    "# each review item becomes a serries of words\n",
    "# so this becomes a list of lists\n",
    "documents = list (read_input (data_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec (documents, window=10, min_count=2, workers=10)\n",
    "model.train(documents,total_examples=len(documents),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = \"dirty\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up top 6 words similar to 'polite'\n",
    "w1 = [\"polite\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up top 6 words similar to 'france'\n",
    "w1 = [\"france\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get everything related to stuff on the bed\n",
    "w1 = [\"bed\",'sheet','pillow']\n",
    "w2 = ['couch']\n",
    "model.wv.most_similar (positive=w1,negative=w2,topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get everything related to stuff on the bed\n",
    "w1 = ['bed']\n",
    "w2 = ['couch']\n",
    "model.wv.most_similar (positive=w1,negative=w2,topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model.wv.doesnt_match([\"cat\",\"dog\",\"france\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model.wv.doesnt_match([\"bed\",\"pillow\",\"duvet\",\"shower\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M, word2Ind = get_matrix_of_vectors(model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncated 2D word2vec matrix\n",
    "M_reduced = reduce_to_k_dim(M, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D visualization\n",
    "words = [\"bad\",\"free\",\"wonderful\",\"clean\",\"quiet\",\"impressive\",\"terrible\",\"good\"]\n",
    "plot_embeddings(M_reduced, word2Ind, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.distance(\"happy\",\"glad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.distance(\"happy\",\"bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analogies: \"man : king :: woman : x\", what is x?\n",
    "pprint.pprint(model.wv.most_similar(positive=['man', 'queen'], negative=['woman']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
