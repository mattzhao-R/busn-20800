{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca98df45",
   "metadata": {},
   "source": [
    "# BUSN 20800 Final project\n",
    "\n",
    "## Due date: 5pm on June 3, 2022 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca22fce6",
   "metadata": {},
   "source": [
    "### Note: \n",
    "\n",
    "This is an individual exam. You need run this notebook on your personal computer because the class server has issue with the package pyLDAvis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5e20f3",
   "metadata": {},
   "source": [
    "# Part 1: Introduction\n",
    "\n",
    "Twitter, one of the world’s largest social media services, has now become a platform for politicians, organizations, and companies to give updates to their followers. Users representing a company or a political party use Twitter to state views on current news, push their political campaigns and even confirm official policy decisions. Important figures that have used the site to broadcast their thoughts to millions of followers include Tesla founder Elon Musk, European Commission President Donald Tusk, and current UK Prime Minister Boris Johnson. The use of social networks can have large financial/business implications — for example, Elon Musk caught a lot of negative attention when he tweeted that ‘funding was secured’ to take his electric vehicle company Tesla private. Tesla’s share price rose as much as 8.5% (1) and this resulted in a punitive investigation into Musk by the Securities and Exchange Commission. This led to Musk being ousted as Chairman for the company that he founded, as well as a $20 million fine — all because of one tweet.\n",
    "\n",
    "In this project, you are requested to analyze Elon Mush's tweet and link his tweets with the stock returns of Tesla  (TSLA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8209caef",
   "metadata": {},
   "source": [
    "Run the following codes to load the data, you don't need to modify any codes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5a3ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up codes\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import spacy \n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import pyLDAvis \n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import re\n",
    "import string\n",
    "import operator\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b220e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv(\"TweetsElonMusk.csv\")\n",
    "df.date = df.date.apply(lambda x: x[:-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3a0d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce8c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out how many tweets we collected\n",
    "text_data = list(df[\"texts\"].values)\n",
    "date      = list(df[\"date\"].values)\n",
    "T         = len(text_data) \n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e4d46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "spacy_nlp   = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4ba0c9",
   "metadata": {},
   "source": [
    "# Part I. Data cleaning and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba79cec",
   "metadata": {},
   "source": [
    "Please follow the data cleaning procedure you have seen in Trump's tweet example. \n",
    "\n",
    "\n",
    "In this ```clean_tweets()``` function, it will clean the original tweet and generate the result with tokenized words.\n",
    "\n",
    "You don't need to modify any codes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a18267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweet):\n",
    "    \"\"\"\n",
    "    Tokenize and lemmatize an input tweet\n",
    "    \n",
    "    Input:\n",
    "        Tweet: string type\n",
    "    Output:\n",
    "        A list containing tokens\n",
    "    \"\"\"\n",
    "    tweet       = re.sub('&amp;', ' ',tweet)\n",
    "    tweet       = emoji_pattern.sub(r' ', tweet)\n",
    "    \n",
    "    word_tokens = spacy_nlp(tweet)\n",
    "    tokens      = []\n",
    "    \n",
    "    for w in word_tokens:\n",
    "        if not w.is_stop: # not stop words\n",
    "            s   = w.text.lower()\n",
    "            s   = re.sub(r'^[@#]', '', s)\n",
    "            s   = re.sub(r'[^a-zA-Z0-9_]+$', '', s)\n",
    "            s   = re.sub(r'https?:\\S*', '', s)\n",
    "            s   = re.sub(r'[-,#()@=!\\\"\\'\\?\\/:]+', ' ', s)\n",
    "            \n",
    "            #replace consecutive non-ASCII characters with a space\n",
    "            s   = re.sub(r'[^\\x00-\\x7F]+',' ', s)\n",
    "            tokens += s.split()\n",
    "    text = \" \".join(tokens)\n",
    "\n",
    "    word_tokens    = spacy_nlp(text)\n",
    "    filtered_tweet = []\n",
    "    for w in word_tokens:\n",
    "        if not w.is_stop:\n",
    "            if w.lemma_ != \"-PRON-\":\n",
    "                s = w.lemma_.lower()\n",
    "            else:\n",
    "                s = w.lower_\n",
    "            s = s.strip('-')\n",
    "            if len(s) <= 1:\n",
    "                continue\n",
    "            if re.match(r'^[a-zA-Z_\\.]+$', s):\n",
    "                filtered_tweet.append(s)\n",
    "    \n",
    "    return filtered_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc37a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emoji patterns\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbfc701",
   "metadata": {},
   "source": [
    "Test if the tokenization works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1698ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "### TODO: Give an example to show the results of tokenization.             ###\n",
    "##############################################################################\n",
    "\n",
    "#Test tokenization\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#                               END OF YOUR CODE                             #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ee5e81",
   "metadata": {},
   "source": [
    "Construct the bag of words model. It stores the times of each word appeared in each tweet.\n",
    "\n",
    "You don't need to modify any codes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f4405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag-of-words model\n",
    "processed_docs = []\n",
    "\n",
    "for t in text_data:\n",
    "    try: \n",
    "        processed_docs.append(clean_tweets(t))\n",
    "    except: pass\n",
    "\n",
    "\n",
    "# create a dictionary for all tweets\n",
    "dictionary     = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "# create word bow for all tweets, this can help us record the times of each word appeared in each tweet.\n",
    "bow_corpus     = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cb3677",
   "metadata": {},
   "source": [
    "# Part II. Latent Dirichlet Allocation(LDA)\n",
    "\n",
    "\n",
    "LDA was proposed in 2003 to infer the topic distribution of documents. It can give the topic of each document in the document set in the form of probability distribution, so that after analyzing some documents to extract their topic distribution, topic clustering or text classification can be performed according to the topic distribution.\n",
    "\n",
    "Latent Dirichlet Allocation (LDA) is a Bayesian probabilistic model of text documents. It assumes a collection of K “topics.” Each topic defines a multinomial distribution over the vocabulary and is assumed to have been drawn from a Dirichlet, $\\beta_{k} \\sim \\text{Dirichlet}(\\eta)$. Given the topics, LDA assumes the following generative process for each document $d$. First, draw a distribution over topics $\\theta_{d} \\sim \\text{Dirichlet}(\\alpha)$. Then, for each word $i$ in the document, draw a topic index $z_{d_i} \\in \\lbrace1, \\dots , K\\rbrace$ from the topic weights $z_{d_i} \\sim \\theta_{d}$ and draw the observed word $w_{d_i}$ from the selected topic, $w_{d_i} \\sim \\beta_{z_{d_i}}$ . For simplicity, we assume symmetric priors on $\\theta$ and $\\beta$, but this assumption is easy to relax.\n",
    "\n",
    "Note that if we sum over the topic assignments $z$, then we get $p(w_{d_i} | \\theta_d, \\beta) = \\sum_{k} \\theta_{dk}\\beta_{kw}$. This\n",
    "leads to the “multinomial PCA” interpretation of LDA; we can think of LDA as a probabilistic factorization of the matrix of word counts $n$ (where $n_{dw}$ is the number of times word $w$ appears in document $d$) into a matrix of topic weights $\\theta$ and a dictionary of topics $\\beta$. Our work can thus be seen as an extension of online matrix factorization techniques that optimize squared error to more general probabilistic formulations.\n",
    "\n",
    "We can analyze a corpus of documents with LDA by examining the posterior distribution of the topics $\\beta$, topic proportions $\\theta$, and topic assignments $z$ conditioned on the documents. This reveals latent structure in the collection that can be used for prediction or data exploration. This posterior cannot be computed directly, and is usually approximated using Markov Chain Monte Carlo\n",
    "(MCMC) methods or variational inference. Both classes of methods are effective, but both present significant computational challenges in the face of massive data sets. Developing scalable approximate inference methods for topic models is an active area of research. \n",
    "\n",
    "This section is sited of [Online Learning for Latent Dirichlet Allocation](https://www.di.ens.fr/~fbach/mdhnips2010.pdf).\n",
    "\n",
    "To learn more about [gensim.models.LadMulticore](https://radimrehurek.com/gensim/models/ldamulticore.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9fcef8",
   "metadata": {},
   "source": [
    "Perform LDA on these tweets and visualize the topic models.\n",
    "\n",
    "(Try different parameters to see the difference.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efafadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running LDA using Bag of Words\n",
    "lda_model = gensim.models.LdaMulticore(corpus=bow_corpus,id2word=dictionary,num_topics=8, passes=20,random_state = 1,workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f363884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the topic models\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    vis_data1 = gensimvis.prepare(lda_model, bow_corpus, dictionary)\n",
    "pyLDAvis.display(vis_data1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0a3485",
   "metadata": {},
   "source": [
    "Based on the visualization results, can you summarize the major topics of Elon Musk's tweets? How many topics would you select?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a275fd",
   "metadata": {},
   "source": [
    "**Your Answer here:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc54762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04185ecf",
   "metadata": {},
   "source": [
    "# Part III. TF-IDF Model (For your reading)\n",
    "\n",
    "TF-IDF means term-frequency-inverse-document-frequency. Term Frequency is the number of times a word has occurred in the document or a word's frequency in a document. Its domain remains local to the document. Document frequency is the fraction of documents in which the word has occurred. It’s calculated based on statistics collected from the entire corpus.\n",
    "\n",
    "In very simple terms TF-IDF scheme is used for extracting features or important words which can be the best representative of your document. To get the intuitive feel of TF-IDF, consider a recipe book which has recipes for various fast foods.\n",
    "\n",
    "Here, the recipe book is our corpus and each recipe is our document. Now consider various recipes such as:\n",
    "1. Burger which will consist of words like \"bun\", \"meat\", \"lettuce\", \"ketchup\", \"heat\", \"onion\", \"food\", \"preparation\", \"delicious\", \"fast\"\n",
    "2. French fries which will consist of words like \"potato\", \"fry\", \"heat\", \"oil\", \"ketchup\", \"food\", \"preparation\", \"fast\"\n",
    "3. Pizza which will consist of words like \"ketchup\", \"capsicum\", \"heat\", \"food\", \"delicious\", \"oregano\", \"dough\", \"fast\"\n",
    "\n",
    "Here words like \"fast\", \"food\", \"preparation\", \"heat\" occur in almost all the recipes. Such words will have a very high document frequency. Now consider words like \"bun\", \"lettuce\" for the recipe burger, \"potato\" and \"fry\" for the recipe french fries and \"dough\" and \"oregano\" for the recipe pizza. These have a high term frequency for the particular recipe they are related to, but will have a comparatively low document frequency.\n",
    "\n",
    "Now we propose the scheme of TF-IDF, which simply put is a mathematical product of term-frequency and the inverse of the document frequency of each term. One can clearly visualize how the words like \"potato\" in French fries will have a high TF-IDF value and at the same time are the best representative of the document which in this case is \"French Fries\".\n",
    "\n",
    "Now mathematically speaking:\n",
    "\n",
    "- **Term frequency**: For a word (or term) $t$, the term frequency is denoted by $\\text{tf}_{t,d}$ and the frequency of each term in a document $d$ is denoted by $\\text{f}_{t,d}$ then $\\text{tf}_{t,d} = \\text{f}_{t,d}$\n",
    "\n",
    "- **Inverse document frequency**: Taking the log smartirs scheme for inverse document frequency idf is calculated as $\\text{idf}(t,D) = \\text{log} \\frac{N}{|\\lbrace d \\in D : t \\in d|}\\;\\;$  or the log of inverse fraction of document that contain the term $t$, where $N$ is the total number of documents in the corpus. To avoid division by zero error generally 1 is added to the denominator.\n",
    "\n",
    "- **TF-IDF**: Finally, TF-IDF is calculated as the product of term frequency $\\text{tf}_{t,d}$ and inverse document frequency $\\text{idf}(t,D)$, $\\text{TF-IDF}(t,d,D) = \\text{tf}_{t,d} * \\text{idf}_{t,D}$\n",
    "\n",
    "This section is sited of [Pivoted document length normalisation](https://rare-technologies.com/pivoted-document-length-normalisation/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe7b336",
   "metadata": {},
   "source": [
    "Run the below cells to construct a TF-IDF model. You don't need to modify any codes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc98117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running LDA using TF-IDF\n",
    "\n",
    "#Create tf-idf model object using models.TfidfModel on bow_corpus and save it to 'tfidf'\n",
    "tfidf           = gensim.models.TfidfModel(bow_corpus)\n",
    "\n",
    "#apply transformation to the entire corpus and name it 'corpus_tfidf'\n",
    "corpus_tfidf    = tfidf[bow_corpus]\n",
    "tfidf_lda_model = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, \n",
    "                                             passes=20, random_state=np.random.RandomState(20800))\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    vis_data_tf_idf   = gensimvis.prepare(tfidf_lda_model, corpus_tfidf, dictionary)\n",
    "\n",
    "pyLDAvis.display(vis_data_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cf379b",
   "metadata": {},
   "source": [
    "Compare the topic models using TF-IDF and vanilla LDA, what differences did you find?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e72fb3",
   "metadata": {},
   "source": [
    "**Your Answer here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7729596",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "966009c8",
   "metadata": {},
   "source": [
    "# Part IV: Sentiments Analysis\n",
    "\n",
    "This function embedds a `SentimentIntensityAnalyzer()`, which is for analyzing the sentiment for each twitter. \n",
    "\n",
    "\n",
    "It returns those tweets which have sentiments scores higher or lower than cutoffs given."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e78983e",
   "metadata": {},
   "source": [
    "You don't need to modify any codes here, but you can try to play with different cutoff values to see how it influences the final outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eddd531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer(sentences, pos_cutoff = 0.7, neg_cutoff = -0.7):\n",
    "    \"\"\"\n",
    "    Given a list of tweets, return those that contain strong positive/negative sentiments\n",
    "    Args:\n",
    "        sentences: a list containing the indices of tweets in text_data\n",
    "        pos_cutoff: tweets with sentiment scores >= pos_cutoff are classified as positive\n",
    "        neg_cutoff: tweets with sentiment scores <= neg_cutoff are classified as negative\n",
    "    Returns:\n",
    "        pos: a list containing the indices of tweets that are classified as positive\n",
    "        neg: a list containing the indices of tweets that are classified as negative\n",
    "    \"\"\"\n",
    "    analyser  = SentimentIntensityAnalyzer()\n",
    "    pos, neg  = [], []\n",
    "    for i in sentences:\n",
    "        score = analyser.polarity_scores(text_data[i])\n",
    "        \n",
    "        if score['compound']   >= pos_cutoff: # positive comments\n",
    "            pos.append(i)\n",
    "        elif score['compound'] <= neg_cutoff: # negative comments\n",
    "            neg.append(i)\n",
    "    return (pos, neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99232d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists containing indices for positive tweets and negative tweets resp.\n",
    "pos, neg = sentiment_analyzer(np.arange(len(text_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61cf03b",
   "metadata": {},
   "source": [
    "Display two tweets with positive and negative sentiment respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03905d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "### TODO: Find a positive example.                                         ###\n",
    "##############################################################################\n",
    "\n",
    "# Positive sentence example\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#                               END OF YOUR CODE                             #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a082d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "### TODO: Find a negative example.                                         ###\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "# Negative sentence example\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#                               END OF YOUR CODE                             #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cf68d9",
   "metadata": {},
   "source": [
    "# Part V. Stock trading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80566553",
   "metadata": {},
   "source": [
    "Now it's time for us to take the above analysis into trading.\n",
    "\n",
    "In this section, we will use TSLA as a trading example. We will use the sentiment we have got before to decide when to long TSLA and short it respectively.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5941df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TSLA return data\n",
    "tsla =  pd.read_csv(\"TSLA.csv\")\n",
    "tsla['CLOSEPRC'] = 0.5 * (tsla.BID + tsla.ASK)\n",
    "tsla.date =tsla.date.apply(lambda x: str(x))\n",
    "tsla.date = tsla.date.apply(lambda x: x[:4]+'-'+x[4:6]+'-'+x[6:])\n",
    "tsla.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcfcbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_range(date):\n",
    "    \"\"\"\n",
    "    Given a date, return the market opening time and closing time on that day\n",
    "    Args:\n",
    "        date: a string of the format 'mm/dd/yy hh:mm'\n",
    "    Returns:\n",
    "        a tuple: '(mm/dd/yy-09:30:00, mm/dd/yy-16:00:00)'\n",
    "    \"\"\"\n",
    "    return (pd.Timestamp(pd.to_datetime(date).date()) + pd.Timedelta('09:30:00'), pd.Timestamp(pd.to_datetime(date).date()) + pd.Timedelta('16:00:00'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c401803f",
   "metadata": {},
   "source": [
    "Here is our trading strategy. \n",
    "\n",
    "\n",
    "For positive sentiment signals, the trading strategy is:\n",
    "+ (1)if signal occurs before market opens: buy at open and sell at close;\n",
    "+ (2)if signal occurs during market hours: buy at close and sell at tomorrow’s close\n",
    "+ (3)if signal occurs after hours: buy at tomorrow’s open and sell at tomorrow’s close\n",
    "\n",
    "For negative sentiment signals, the trading strategy is:\n",
    "+ (1)if signal occurs before market opens: sell at open and return to 100% SPY exposure at close\n",
    "+ (2)if signal occurs during market hours: sell at close and return to 100% SPY exposure at tomorrow’s close\n",
    "+ (3)if signal occurs after hours: sell at tomorrow’s open and return to 100% SPY exposure at tomorrow’s close\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1c1a6b",
   "metadata": {},
   "source": [
    "We have implemented this trading strategy for you. You can take a look in the below ```trade``` function.\n",
    "\n",
    "\n",
    "\n",
    "You don't need to modify any codes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1935347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trade(signals, positive=True):\n",
    "    \"\"\"\n",
    "    Given a list of signals, compute the profits gained.\n",
    "    For positive sentiment signals, the trading strategy is:\n",
    "        (1)if signal occurs before market opens: buy at open and sell at close;\n",
    "        (2)if signal occurs during market hours: buy at close and sell at tomorrow’s close\n",
    "        (3)if signal occurs after hours: buy at tomorrow’s open and sell at tomorrow’s close\n",
    "    For negative sentiment signals, the trading strategy is:\n",
    "        (1)if signal occurs before market opens: sell at open and return to 100% SPY exposure at close\n",
    "        (2)if signal occurs during market hours: sell at close and return to 100% SPY exposure at tomorrow’s close\n",
    "        (3)if signal occurs after hours: sell at tomorrow’s open and return to 100% SPY exposure at tomorrow’s close\n",
    "    Args:\n",
    "        signals: a list containing the time ('mm/dd/yy hh:mm') when a signal occurred (aka. tweet was published)\n",
    "        positive: boolean. True means the input are all positive sentiment signals; False means negative sentiment signals. \n",
    "    Returns:\n",
    "        No return value. Modifications are done on the dict 'profits'\n",
    "    \"\"\"\n",
    "    for s in signals:\n",
    "        op, ed = time_range(s)\n",
    "        x      = str(pd.to_datetime(s).date())\n",
    "        #Signal occurs before market opens:\n",
    "        if pd.Timestamp(s) < op and len(tsla.loc[tsla[\"date\"] == x]) > 0:\n",
    "            if positive:\n",
    "                #buy at open and sell at close\n",
    "                p = tsla.loc[tsla[\"date\"] == x][\"CLOSEPRC\"].values[0] - tsla.loc[tsla[\"date\"] == x][\"OPENPRC\"].values[0]\n",
    "            else:\n",
    "                #sell at open and return to 100% SPY exposure at close\n",
    "                p = tsla.loc[tsla[\"date\"] == x][\"OPENPRC\"].values[0] - tsla.loc[tsla[\"date\"] == x][\"CLOSEPRC\"].values[0]\n",
    "            profits[x].append(p)\n",
    "        else:\n",
    "            t = pd.Timestamp(pd.to_datetime(s).date()) + pd.Timedelta('1 days') #next day\n",
    "            y = str(pd.to_datetime(t).date())\n",
    "            if len(tsla.loc[tsla[\"date\"] == y]) > 0:\n",
    "                #Signal occurs after hours:\n",
    "                if pd.Timestamp(s) > ed:\n",
    "                    if positive:\n",
    "                        #buy at tomorrow’s open and sell at tomorrow’s close\n",
    "                        p = tsla.loc[tsla[\"date\"] == y][\"CLOSEPRC\"].values[0] - tsla.loc[tsla[\"date\"] == y][\"OPENPRC\"].values[0]\n",
    "                    else:\n",
    "                        #sell at tomorrow’s open and return to 100% SPY exposure at tomorrow’s close\n",
    "                        p = tsla.loc[tsla[\"date\"] == y][\"OPENPRC\"].values[0] - tsla.loc[tsla[\"date\"] == y][\"CLOSEPRC\"].values[0]\n",
    "                # Signal occurs during market hours:\n",
    "                elif len(tsla.loc[tsla[\"date\"] == x]) > 0:\n",
    "                    if positive:\n",
    "                        #buy at close and sell at tomorrow’s close\n",
    "                        p = tsla.loc[tsla[\"date\"] == y][\"CLOSEPRC\"].values[0] - tsla.loc[tsla[\"date\"] == x][\"CLOSEPRC\"].values[0]\n",
    "                    else:\n",
    "                        #sell at close and return to 100% SPY exposure at tomorrow’s close\n",
    "                        p = tsla.loc[tsla[\"date\"] == x][\"CLOSEPRC\"].values[0] - tsla.loc[tsla[\"date\"] == y][\"CLOSEPRC\"].values[0]\n",
    "                profits[y].append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4fe2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_signals(signals):\n",
    "    \"\"\"\n",
    "    Given a list of signals, return the dates of their occurrences and the closing values of TSLA at the corresponding dates\n",
    "    Args:\n",
    "        signals: a list containing the time ('mm/dd/yy hh:mm') when a signal occurred (aka. tweet was published)\n",
    "    Returns:\n",
    "        time: a list containing the date ('mm/dd/yy') of each signal\n",
    "        value: a list containing the closing values of TSLA at each day in time\n",
    "    \"\"\"\n",
    "    time, value = [], []\n",
    "    for s in signals:\n",
    "        x = str(pd.to_datetime(s).date())\n",
    "        if len(tsla.loc[tsla[\"date\"] == x]) > 0:\n",
    "            time.append(x)\n",
    "            value.append(tsla.loc[tsla[\"date\"] == x][\"CLOSEPRC\"].values[0])\n",
    "    return (time, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8300bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the profits, defalut = []\n",
    "\n",
    "profits = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4535e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the positive signals and negative signals and perform the trading respectively.\n",
    "\n",
    "pos_time, neg_time = [df.iloc[i][\"date\"] for i in pos], [df.iloc[i][\"date\"] for i in neg]\n",
    "\n",
    "\n",
    "trade(pos_time)\n",
    "trade(neg_time, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d66c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "time  = list(tsla[\"date\"].values)\n",
    "y1    = list(tsla[\"CLOSEPRC\"].values)\n",
    "y2    = [] \n",
    "start = 0\n",
    "for t, v in zip(time, y1):\n",
    "    start += np.mean(profits[t] if profits[t] else 0.0)\n",
    "    y2.append(v + start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b233f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the buy and sell signals\n",
    "\n",
    "buy, bval  = find_signals(pos_time)\n",
    "sell, sval = find_signals(neg_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d9882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the trading strategy\n",
    "\n",
    "plt.figure(figsize = (16, 8))\n",
    "plt.plot(time, y1, label = \"Tsla raw return\")\n",
    "plt.plot(time, y2, label = \"Portfolio Value\")\n",
    "plt.scatter(buy, bval, marker = 'v', c = \"green\", label = \"Buy Signal\")\n",
    "plt.scatter(sell, sval, marker = '^', c = \"red\", label = \"Sell Signal\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.legend(loc = \"upper left\")\n",
    "\n",
    "plt.xticks(range(1,2960,300))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb53dc0",
   "metadata": {},
   "source": [
    "Based on the visualization above, do you think trade based on sentiment is a good idea? Analyze the returns and report your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c3ed53",
   "metadata": {},
   "source": [
    "**Your Answer here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9a6973",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
