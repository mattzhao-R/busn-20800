{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21b49c0f",
   "metadata": {},
   "source": [
    "# Assignment 6: Neural network\n",
    "\n",
    "## BUS 20800: Big Data\n",
    "## Due: 11:59 am on May 23, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8418b5",
   "metadata": {},
   "source": [
    "This assignment is a simplified exercise based on the simulations in this paper: \n",
    "[Empirical Asset Pricing via Machine Learning](https://dachxiu.chicagobooth.edu/download/ML.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667aa6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot  as plt\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733bd154",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Simu/SimuData_100/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75da131",
   "metadata": {},
   "source": [
    "# Part I. DGP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8382a64",
   "metadata": {},
   "source": [
    "You can check the data generating process in `DGP.py`. We have generated firm characteristics and returns as linear and nonlinear functions of these characteristics, respectively.\n",
    "\n",
    "\n",
    "It takes several minutues to run the simulation code and generate the data, if you want to run it yourself. We have provided the simulated data for you. You can check the simulated data in `Simu` folder. There are 100 Monte Carlo repetions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1244f8e",
   "metadata": {},
   "source": [
    "# Part II. Two Layer NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21350222",
   "metadata": {},
   "source": [
    "First, follow the instruction codes in lecture codes, finish this toy `TwoLayerNet` class for rergression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce21904",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(object):\n",
    "    \"\"\"\n",
    "    A two-layer fully-connected neural network. The net has an input dimension of\n",
    "    N, a hidden layer dimension of H, and performs regression task.\n",
    "    \n",
    "    We train the network with square loss and L2 regularization on the\n",
    "    weight matrices. The network uses a ReLU nonlinearity after the first fully\n",
    "    connected layer.\n",
    "\n",
    "    In other words, the network has the following architecture:\n",
    "\n",
    "    input - fully connected layer - ReLU - fully connected layer - square loss\n",
    "\n",
    "    The outputs of the second fully-connected layer are the predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Initialize the NN\n",
    "    def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initialize the model. Weights are initialized to small random values and\n",
    "        biases are initialized to zero. Weights and biases are stored in the\n",
    "        variable self.params, which is a dictionary with the following keys:\n",
    "\n",
    "        W1: First layer weights; Shape (D, H)\n",
    "        b1: First layer biases; Shape (H,)\n",
    "        W2: Second layer weights; Shape (H, 1)\n",
    "        b2: Second layer biases; Shape (1,)\n",
    "\n",
    "        Inputs:\n",
    "        - input_size: The dimension D of the input data.\n",
    "        - hidden_size: The number of neurons H in the hidden layer.\n",
    "        - output_size: Output prediction.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W1'] = std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def loss(self, X, y=None, reg=0.0):\n",
    "        \"\"\"\n",
    "        Compute the loss and gradients for a two layer fully connected neural\n",
    "        network.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Shape (N, D). Each X[i] is a training sample.\n",
    "        - y: Vector of training labels.\n",
    "        - reg: Regularization strength.\n",
    "\n",
    "        Returns:\n",
    "        If y is None, return a matrix scores of shape (N, 1) where scores[i,0] is\n",
    "        the prediction.\n",
    "\n",
    "        If y is not None, instead return a tuple of:\n",
    "        - loss: Loss (data loss and regularization penalty) \n",
    "        - grads: Dictionary mapping parameter names to gradients of those parameters\n",
    "          with respect to the loss function; has the same keys as self.params.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Unpack variables from the params dictionary\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        N, D = X.shape\n",
    "\n",
    "        # Compute the forward pass\n",
    "        # Use Relu as the non-linearity function\n",
    "        scores = np.maximum(X.dot(W1) + b1, 0).dot(W2) + b2\n",
    "\n",
    "\n",
    "        # If the targets are not given then jump out, we're done\n",
    "        if y is None:\n",
    "            return scores\n",
    "        \n",
    "##############################################################################\n",
    "### TODO: Calculate the loss function (data + regularization)              ###\n",
    "##############################################################################\n",
    "        # Compute the loss\n",
    "        loss = None \n",
    "        # Add regularization loss\n",
    "        loss  =+ None\n",
    "##############################################################################\n",
    "#                               END OF YOUR CODE                             #\n",
    "##############################################################################\n",
    "\n",
    "        \n",
    "        \n",
    "        # Backward pass: compute gradients\n",
    "        grads = {}            \n",
    "##############################################################################\n",
    "### TODO: Calculate the gradient of each parameter.                        ###\n",
    "##############################################################################\n",
    "        grads['b2'] = None\n",
    "        grads['W2'] = None\n",
    "        grads['b1'] = None\n",
    "        grads['W1'] = None\n",
    "##############################################################################\n",
    "#                               END OF YOUR CODE                             #\n",
    "##############################################################################\n",
    "        \n",
    "        return loss, grads\n",
    "\n",
    "    def train(self, X, y, X_val, y_val,\n",
    "              learning_rate=1e-3, learning_rate_decay=0.95,\n",
    "              reg=5e-6, num_iters=100,\n",
    "              batch_size=200, verbose=False):\n",
    "        \"\"\"\n",
    "        Train this neural network using SGD.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Shape (N, D) \n",
    "        - y: Shape (N, )\n",
    "        - X_val: Shape (N_val, D) \n",
    "        - y_val: Shape (N_val, ) \n",
    "        - learning_rate: Scalar giving learning rate for optimization.\n",
    "        - learning_rate_decay: Scalar giving factor used to decay the learning rate\n",
    "          after each epoch.\n",
    "        - reg: Scalar giving regularization strength.\n",
    "        - num_iters: Number of steps to take when optimizing.\n",
    "        - batch_size: Number of training examples to use per step.\n",
    "        - verbose: boolean; if true print progress during optimization.\n",
    "        \"\"\"\n",
    "        \n",
    "        num_train = X.shape[0]\n",
    "        iterations_per_epoch = max(num_train / batch_size, 1)\n",
    "\n",
    "        # Use SGD to optimize the parameters in self.model\n",
    "        loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "\n",
    "            mask  = np.random.choice(num_train, batch_size, replace=True)\n",
    "            X_batch = X[mask]\n",
    "            y_batch = y[mask]\n",
    "\n",
    "            # Compute loss and gradients using the current minibatch\n",
    "            loss, grads = self.loss(X_batch, y=y_batch, reg=reg)\n",
    "            loss_history.append(loss)\n",
    "\n",
    "            # Gradient descent \n",
    "##############################################################################\n",
    "### TODO: Perform Gradient descent to update parameters                                    ###\n",
    "##############################################################################\n",
    "##############################################################################\n",
    "            self.params['b1']  += None\n",
    "            self.params['W1']  += None\n",
    "            self.params['b2']  += None\n",
    "            self.params['W2']  += None\n",
    "\n",
    "##############################################################################\n",
    "#                               END OF YOUR CODE                             #\n",
    "##############################################################################\n",
    "\n",
    "            if verbose and it % 100 == 0:\n",
    "                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "            # Every epoch, check train and val accuracy and decay learning rate.\n",
    "            if it % iterations_per_epoch == 0:\n",
    "                # Check accuracy\n",
    "                train_acc = (self.predict(X_batch) == y_batch).mean()\n",
    "                val_acc = (self.predict(X_val) == y_val).mean()\n",
    "                train_acc_history.append(train_acc)\n",
    "                val_acc_history.append(val_acc)\n",
    "\n",
    "                # Decay learning rate\n",
    "                learning_rate *= learning_rate_decay\n",
    "\n",
    "        return {\n",
    "          'loss_history': loss_history,\n",
    "          'train_acc_history': train_acc_history,\n",
    "          'val_acc_history': val_acc_history,\n",
    "        }\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this two-layer network to predict return for\n",
    "        data points.\n",
    "        Inputs:\n",
    "        - X: Shape (N, D)\n",
    "\n",
    "        Returns:\n",
    "        - y_pred: Shape (N,)\n",
    "        \"\"\"\n",
    "\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        \n",
    "##############################################################################\n",
    "### TODO: Calculate the prediction.                                        ###\n",
    "##############################################################################\n",
    "        y_pred = None\n",
    "\n",
    "##############################################################################\n",
    "#                               END OF YOUR CODE                             #\n",
    "##############################################################################\n",
    "        \n",
    "        return y_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95738ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small net and some toy data to check your implementations.\n",
    "# Note that we set the random seed for repeatable experiments.\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model():\n",
    "    np.random.seed(0)\n",
    "    return TwoLayerNet(input_size, hidden_size, output_size, std=1e-1)\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    y = np.array([0, 1, 2, 2, 1]).reshape(-1,1)\n",
    "    return X, y\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8bcf63",
   "metadata": {},
   "source": [
    "## Forward pass: compute scores\n",
    "Open the file `neural_net.py` and look at the method `TwoLayerNet.loss`. It takes the data and weights and computes the class scores, the loss, and the gradients with respect to the parameters. \n",
    "\n",
    "Implement the first part of the forward pass which uses the weights and biases to compute the scores for all inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f32643",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = net.loss(X)\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print()\n",
    "print('correct scores:')\n",
    "correct_scores = np.asarray([\n",
    "  [0.38061523],\n",
    "  [-0.55607245],\n",
    "  [-0.59991751],\n",
    "  [-0.61246639],\n",
    "  [0.05788525]])\n",
    "print(correct_scores)\n",
    "print()\n",
    "\n",
    "# The difference should be very small. We get < 1e-7\n",
    "print('Difference between your scores and correct scores:')\n",
    "print(np.sum(np.abs(scores - correct_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba24572",
   "metadata": {},
   "source": [
    "## Forward pass: compute loss\n",
    "In the same function, implement the second part that computes the data and regularizaion loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9773f227",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, _ = net.loss(X, y.reshape(-1,1), reg=0.05)\n",
    "correct_loss = 3.439836236605\n",
    "\n",
    "# should be very small, we get < 1e-12\n",
    "print('Difference between your loss and correct loss:')\n",
    "print(np.sum(np.abs(loss - correct_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42489c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_check import eval_numerical_gradient\n",
    "\n",
    "# Use numeric gradient checking to check your implementation of the backward pass.\n",
    "# The difference between the numeric and analytic gradients should be less than \n",
    "# 1e-8 for each of W1, W2, b1, and b2.\n",
    "\n",
    "loss, grads = net.loss(X, y, reg=0.05)\n",
    "\n",
    "# these should all be less than 1e-8 or so\n",
    "for param_name in grads:\n",
    "    f = lambda W: net.loss(X, y, reg=0.05)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "    print('%s max relative error: %e' % (param_name, rel_error(param_grad_num, grads[param_name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0bac03",
   "metadata": {},
   "source": [
    "## Combine the above: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba59195",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = init_toy_model()\n",
    "stats = net.train(X, y, X, y,\n",
    "            learning_rate=1e-1, reg=5e-6,\n",
    "            num_iters=100, verbose=False)\n",
    "\n",
    "print('Final training loss: ', stats['loss_history'][-1])\n",
    "\n",
    "# plot the loss history\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('iteration',size = 20)\n",
    "plt.ylabel('training loss',size = 20)\n",
    "plt.title('Training Loss history', size = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d8b10a",
   "metadata": {},
   "source": [
    "#  Part III. Multi-layer NN\n",
    "\n",
    "Now we have finished the top layer, and we want to build networks using a more modular design so that we can implement different layer types in isolation and then snap them together into models with various architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1188ea64",
   "metadata": {},
   "source": [
    "## Affine layer: foward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d8a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). \n",
    "    Reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "\n",
    "##############################################################################\n",
    "### TODO: Perform affine transformation.                                   ###\n",
    "##############################################################################\n",
    "    out = None\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#                               END OF YOUR CODE                             #\n",
    "##############################################################################\n",
    "    cache = (x, w, b)\n",
    "    \n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c82ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 1\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.51506276],\n",
    "                        [ 3.32259414]])\n",
    "\n",
    "# Compare your output with ours. The error should be around e-9 or less.\n",
    "print('Testing affine_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a87bfc",
   "metadata": {},
   "source": [
    "## Affine layer: backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e84f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "      - x: Input data, of shape (N, d_1, ... d_k)\n",
    "      - w: Weights, of shape (D, M)\n",
    "      - b: Biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "\n",
    "##############################################################################\n",
    "### TODO: Calculate the gradient of affine transformation                  ###\n",
    "##############################################################################\n",
    "\n",
    "    db = None\n",
    "    dw = None\n",
    "    dx = None\n",
    "    \n",
    "\n",
    "##############################################################################\n",
    "#                               END OF YOUR CODE                             #\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf6685",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "\n",
    "\n",
    "# Test the affine_backward function\n",
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 1)\n",
    "b = np.random.randn(1)\n",
    "dout = np.random.randn(10, 1)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around e-10 or less\n",
    "print('Testing affine_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dw error: ', rel_error(dw_num, dw))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d9c823",
   "metadata": {},
   "source": [
    "##  ReLU activation: forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050e7499",
   "metadata": {},
   "source": [
    "### ReLU\n",
    "There are many potential choices for the nonlinear activation function (such\n",
    "as sigmoid, hyperbolic, softmax). We use the same activation function at all\n",
    "nodes, and choose a popular functional form in recent literature known as the\n",
    "rectified linear unit (ReLU), defined as:\n",
    "\n",
    "$$\\text{ReLU}(x) = \\begin{cases}0 \\quad &\\text{if}\\; x < 0 \\\\\n",
    "x \\quad &\\text{otherwise} \\end{cases} $$\n",
    "\n",
    "which encourages sparsity in the number of active neurons and allows for faster derivative evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1a9eef",
   "metadata": {},
   "source": [
    "Our neural network model has the following general formula. Let $K^{(l)}$ denote the number of neurons in each layer $l =1,...,L$. Define the output of neuron $k$ in layer $l$ as $x_{k}^{(l)}$. Next, define the vector of outputs for this layer (augmented to include a constant, $x_{0}^{(l)}$ ) as \n",
    "$x^{(l)} = (1,x_{1}^{(l)},...,x_{K^{l}}^{(l)})'$. \n",
    "To initialize the network, similarly define the input layer using the raw predictors, $x^{(0)} =(1,z_{1},...,z_{N})'$. The recursive output formula for the neural network at each neuron in layer $l>0$ is then\n",
    "\n",
    "$$x_{k}^{(l)} =  \\text{ReLU} \\left( x^{(l-1)'} \\;\\theta_{k}^{(l-1)}\\; \\right)$$\n",
    "\n",
    "with final output:\n",
    "\n",
    "$$g(z;\\theta) = x^{(L-1)'} \\;\\theta^{L-1}$$\n",
    "\n",
    "The number of weight parameters in each hidden layer $l$ is $K^{(l)}\\left(1+K^{(l−1)} \\;\\right)$, plus another $1+K^{(L−1)}$ weights for the output layer.\n",
    "\n",
    "We estimate the neural network weight parameters by minimizing the penalized $l_{2}$ objective function of prediction errors. Unlike tree-based algorithms that require “greedy” optimization, training a neural network, in principle, allows for joint updates of all model parameters at each step of the optimization—a substantial advantage of neural networks over trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b568124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "    \"\"\"\n",
    "##############################################################################\n",
    "### TODO: Calculate the gradient of affine transformation                  ###\n",
    "##############################################################################\n",
    "    out = None\n",
    "##############################################################################\n",
    "#                               END OF YOUR CODE                             #\n",
    "##############################################################################\n",
    "    cache = x\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f4b15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be on the order of e-8\n",
    "print('Testing relu_forward function:')\n",
    "print('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eda54c8",
   "metadata": {},
   "source": [
    "## ReLU activation: backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade5d0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of ReLUs.\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    dx, x = None, cache\n",
    "    \n",
    "##############################################################################\n",
    "### TODO: Calculate the gradient of affine transformation                  ###\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "    dx = None\n",
    "##############################################################################\n",
    "#                               END OF YOUR CODE                             #\n",
    "##############################################################################\n",
    "    cache = x\n",
    "\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f8bcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be on the order of e-12\n",
    "print('Testing relu_backward function:')\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c944a376",
   "metadata": {},
   "source": [
    "## Loss layers: MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5446cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_square_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for mean square loss.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Shape (N, ) \n",
    "    - y: Shape (N,)\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    \n",
    "##############################################################################\n",
    "### TODO: Calculate the loss and gradient for MSE.                         ###\n",
    "##############################################################################\n",
    "    \n",
    "    loss = None\n",
    "    dx = None\n",
    "    \n",
    "\n",
    "##############################################################################\n",
    "#                               END OF YOUR CODE                             #\n",
    "##############################################################################\n",
    "\n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b25a28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "num_output, num_inputs = 1, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_output)\n",
    "y = np.random.randint(num_output, size=num_inputs).reshape(-1,1)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: mean_square_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = mean_square_loss(x, y)\n",
    "\n",
    "# Test mean_square_loss function. Loss should be close to 2.3 and dx error should be around e-8\n",
    "print('\\nTesting softmax_loss:')\n",
    "print('loss: ', loss)\n",
    "print('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772fcff3",
   "metadata": {},
   "source": [
    "## Combine the above blocks and build a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d67722",
   "metadata": {},
   "source": [
    "## Initial loss and gradient check\n",
    "\n",
    "As a sanity check, run the following to check the initial loss and to gradient check the network both with and without regularization. Do the initial losses seem reasonable?\n",
    "\n",
    "For gradient checking, you should expect to see errors around 1e-7 or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad1289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedNet(object):\n",
    "    \"\"\"\n",
    "    A fully-connected neural network with an arbitrary number of hidden layers,\n",
    "    ReLU nonlinearities, and a mean square loss function. For a network with L layers,\n",
    "    the architecture will be\n",
    "\n",
    "    {affine - [batch/layer norm] - relu - [dropout]} x (L - 1) - affine - mean_square\n",
    "\n",
    "    where batch/layer normalization and dropout are optional, and the {...} block is\n",
    "    repeated L - 1 times.\n",
    "\n",
    "    Learnable parameters are stored in the\n",
    "    self.params dictionary and will be learned using the Solver class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dims, input_dim=100, num_output=1,\n",
    "                 reg=0.0,\n",
    "                 weight_scale=1e-2, dtype=np.float32):\n",
    "        \"\"\"\n",
    "        Initialize a new FullyConnectedNet.\n",
    "\n",
    "        Inputs:\n",
    "        - hidden_dims: A list of integers giving the size of each hidden layer.\n",
    "        - input_dim: An integer giving the size of the input.\n",
    "        - num_output: For regression, num_output = 1\n",
    "        - reg: Scalar giving L2 regularization strength.\n",
    "        - weight_scale: Scalar giving the standard deviation for random\n",
    "          initialization of the weights.\n",
    "        - dtype: A numpy datatype object; all computations will be performed using\n",
    "          this datatype. float32 is faster but less accurate, so you should use\n",
    "          float64 for numeric gradient checking.\n",
    "\n",
    "        \"\"\"\n",
    "        self.reg = reg\n",
    "        self.num_layers = 1 + len(hidden_dims)\n",
    "        self.dtype = dtype\n",
    "        self.params = {}\n",
    "\n",
    "        if len(hidden_dims)>0:\n",
    "            self.params['W1'] = weight_scale * np.random.randn(input_dim, hidden_dims[0])\n",
    "            self.params['b1'] = np.zeros(hidden_dims[0])\n",
    "        \n",
    "        for idx, dim in enumerate(hidden_dims):\n",
    "            if idx < len(hidden_dims)-1:\n",
    "                self.params['W' + str(2 + idx)] = weight_scale * np.random.randn(dim, hidden_dims[idx + 1])\n",
    "                self.params['b' + str(2 + idx)] = np.zeros(hidden_dims[idx + 1])\n",
    "            else:\n",
    "                self.params['W' + str(2 + idx)] = weight_scale * np.random.randn(hidden_dims[idx], num_output)\n",
    "                self.params['b' + str(2 + idx)] = np.zeros(num_output)\n",
    "\n",
    "\n",
    "        # Cast all parameters to the correct datatype\n",
    "        for k, v in self.params.items():\n",
    "            self.params[k] = v.astype(dtype)\n",
    "\n",
    "\n",
    "    def loss(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Compute loss and gradient for the fully-connected net.\n",
    "\n",
    "        Input / output: Same as TwoLayerNet above.\n",
    "        \"\"\"\n",
    "        X = X.astype(self.dtype)\n",
    "        mode = 'test' if y is None else 'train'\n",
    "\n",
    "        out = X\n",
    "        for la in range(self.num_layers-1):\n",
    "            a, _      = affine_forward(out, self.params['W'+str(la+1)], self.params['b'+str(la+1)])\n",
    "            out, _    = relu_forward(a)\n",
    "            \n",
    "\n",
    "            \n",
    "        scores, _ = affine_forward(out,\n",
    "                                   self.params['W'+str(self.num_layers)],self.params['b'+str(self.num_layers)])\n",
    "\n",
    "\n",
    "        # If test mode return early\n",
    "        if mode == 'test':\n",
    "            return scores\n",
    "\n",
    "        out = X\n",
    "        a=[]\n",
    "        affine_cache = []\n",
    "        relu_cache   = []\n",
    "       \n",
    "        \n",
    "        loss, grads = 0.0, {}\n",
    "        \n",
    "        for la in range(self.num_layers-1):\n",
    "            a, a_cache    = affine_forward(out, self.params['W'+str(la+1)], self.params['b'+str(la+1)])\n",
    "            affine_cache.append(a_cache)\n",
    "                \n",
    "            out, r_cache    = relu_forward(a)\n",
    "            relu_cache.append(r_cache)\n",
    "            \n",
    "   \n",
    "        scores, cache  = affine_forward(out, self.params['W'+str(self.num_layers)], self.params['b'+str(self.num_layers)])\n",
    "        loss, dx       = mean_square_loss(scores, y)\n",
    "        \n",
    "        \n",
    "    \n",
    "        da, dW, db  = affine_backward(dx, cache)\n",
    "        grads['W'+str(self.num_layers)] = dW\n",
    "        grads['b'+str(self.num_layers)] = db\n",
    "        \n",
    "            \n",
    "        for la in reversed(range(self.num_layers-1)):            \n",
    "            \n",
    "            dr = relu_backward(da, relu_cache[la])\n",
    "            \n",
    "            da, dW, db = affine_backward(dr, affine_cache[la])\n",
    "            grads['W'+ str(la+1)] = dW\n",
    "            grads['b'+ str(la+1)] = db\n",
    "            \n",
    "        for la in range(self.num_layers):\n",
    "            loss += 1/2*self.reg * np.sum(self.params['W'+ str(la+1)] * self.params['W'+ str(la+1)])     \n",
    "            grads['W'+ str(la+1)]  +=  self.reg*self.params['W'+ str(la+1)] \n",
    "\n",
    "        return loss, grads\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b34e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(231)\n",
    "N, D, H1, H2, C = 2, 15, 20, 30, 1\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,)).reshape(-1,1)\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "  print('Running check with reg = ', reg)\n",
    "  model = FullyConnectedNet([H1, H2], input_dim=D, num_output=C,\n",
    "                            reg=reg, weight_scale=5e-2, dtype=np.float64)\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print('Initial loss: ', loss)\n",
    "  \n",
    "  # Most of the errors should be on the order of e-7 or smaller.   \n",
    "  # for the check when reg = 0.0\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "    print('%s relative error: %.2e' % (name, rel_error(grad_num, grads[name])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3d6922",
   "metadata": {},
   "source": [
    "# Part IV. Return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95156cc4",
   "metadata": {},
   "source": [
    "The simulation result in `Simu` file. It contains 100 time Monte Carlo simulation experiment, each with firm characteristics, linear and non-linear return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe6b5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup codes\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa166acd",
   "metadata": {},
   "source": [
    "Run the following cells to load the data. You can choose different i here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3df74dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = str(100)\n",
    "\n",
    "factor = np.array(pd.read_csv(path+'c'+i+'.csv'))\n",
    "linear_ret = np.array(pd.read_csv(path+'r1_'+i+'_1.csv'))\n",
    "nonlinear_ret = np.array(pd.read_csv(path+'r2_'+i+'_1.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af17f754",
   "metadata": {},
   "source": [
    "With the above preparation, build a NN using linear and non-linear return. And compare the results with \n",
    "+ OLS, \n",
    "+ OLS + Regularization (L1,L2),\n",
    "+ Tree model(CART+Random Forest).\n",
    "\n",
    "What conclusions can you make? (You can use some visualziation tools to help you illustrate your results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a939e2",
   "metadata": {},
   "source": [
    "### Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c522285",
   "metadata": {},
   "source": [
    "Here is a NN example with subsample dataset. Once you finish all the cells above. You can directly run the below cells to get the prediction result.You don't need to modify any codes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654c07ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN with sample data\n",
    "\n",
    "from solver import Solver\n",
    "\n",
    "num_train = 2500\n",
    "num_valid = 500\n",
    "small_data = {\n",
    "  'X_train': factor[:num_train],\n",
    "  'y_train': nonlinear_ret[:num_train],\n",
    "  'X_val': factor[num_train:num_train+num_valid],\n",
    "  'y_val': nonlinear_ret[num_train:num_train+num_valid]\n",
    "}\n",
    "\n",
    "learning_rate = 5e-2  # Experiment with this!\n",
    "weight_scale = 1*1e-1   # Experiment with this!\n",
    "model = FullyConnectedNet([100, 100, 100, 100],\n",
    "                weight_scale=weight_scale, dtype=np.float64)\n",
    "solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "         )\n",
    "solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35543e61",
   "metadata": {},
   "source": [
    "Now follow the same framework shown in the subsample data, train the NN with full sample data.Try to tune the parameters and construct the neural network with different numbers of layers. Compare their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3f4d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN with full data\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "### TODO: NN with full data.                                               ###\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#                               END OF YOUR CODE                             #\n",
    "##############################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d126a76",
   "metadata": {},
   "source": [
    "### Linear model and its variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3a2e09",
   "metadata": {},
   "source": [
    "Construct the linear regression model, with different regularization method, perform the above prediction task. Compare their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b53380",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "### TODO: Linear model and its variants.                                   ###\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#                               END OF YOUR CODE                             #\n",
    "##############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7004f7f",
   "metadata": {},
   "source": [
    "### Tree based model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0c1e16",
   "metadata": {},
   "source": [
    "Now construct the tree model (CART, Random Forest) and redo the above exercise. Compare their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f55157",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "### TODO: Tree based models.                                               ###\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "#                               END OF YOUR CODE                             #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6106c19f",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Compare the above findings and write your results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d1b48c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
